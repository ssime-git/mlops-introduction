{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Integration for Model Serving and Registry Management\n",
    "\n",
    "In this notebook, we delve into advanced aspects of MLflow, focusing on model serving, inference, and the management of model versions in the MLflow model registry. Our goal is to demonstrate how MLflow supports the operational phase of the machine learning lifecycle, which includes serving models for inference and efficiently managing multiple versions of models.\n",
    "\n",
    "We will explore the practical application of these concepts using a text classification model. This will include loading models for inference, performing predictions, managing different versions of models, and understanding how to transition models through various stages in the model lifecycle. These skills are essential for operational efficiency and effective model management in real-world machine learning applications, aligning with the core themes of our course on MLops and experiment tracking.\n",
    "\n",
    "\n",
    "### Objective:\n",
    "* Loading and Serving Models\n",
    "* Inference with the Model\n",
    "* Managing Model Versions\n",
    "* Deleting Models and Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Ensure all necessary libraries are installed and imported for our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Import necessary libraries focusing on MLflow for model retrieval, PyTorch for model operations, and Transformers for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastiensime/Documents/mlops-introduction/mlflow_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Mlflow Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = mlflow.tracking.MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Model from MLflow\n",
    "\n",
    "In this step, we'll explore two methods to retrieve our trained model from MLflow. Understanding the nuances of each method is key to making an informed choice in a real-life scenario based on the requirements and constraints of your deployment environment.\n",
    "\n",
    "#### Method 1: Using the Built-in PyTorch Loader\n",
    "\n",
    "This method is straightforward and uses MLflow's built-in functionality to load PyTorch models. It's user-friendly and works well when you're working within a PyTorch-centric workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sebastiensime/Documents/mlops-introduction/artifact_store/2/70aed8e7fafd493a8433054a2340ba05/artifacts/model/MLmodel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m model_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# or \"production\", \"staging\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels:/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/mlops-introduction/mlflow_env/lib/python3.12/site-packages/mlflow/pytorch/__init__.py:683\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_uri, dst_path, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    682\u001b[0m local_model_path \u001b[38;5;241m=\u001b[39m _download_artifact_from_uri(artifact_uri\u001b[38;5;241m=\u001b[39mmodel_uri, output_path\u001b[38;5;241m=\u001b[39mdst_path)\n\u001b[0;32m--> 683\u001b[0m pytorch_conf \u001b[38;5;241m=\u001b[39m \u001b[43m_get_flavor_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflavor_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLAVOR_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m _add_code_from_conf_to_system_path(local_model_path, pytorch_conf)\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m__version__ \u001b[38;5;241m!=\u001b[39m pytorch_conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_version\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/mlops-introduction/mlflow_env/lib/python3.12/site-packages/mlflow/utils/model_utils.py:61\u001b[0m, in \u001b[0;36m_get_flavor_configuration\u001b[0;34m(model_path, flavor_name)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Obtains the configuration for the specified flavor from the specified\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mMLflow model path. If the model does not contain the specified flavor,\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03man exception will be thrown.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflavors[flavor_name]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel does not have the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflavor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m flavor\u001b[39m\u001b[38;5;124m'\u001b[39m, RESOURCE_DOES_NOT_EXIST\n\u001b[1;32m     65\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/mlops-introduction/mlflow_env/lib/python3.12/site-packages/mlflow/models/model.py:596\u001b[0m, in \u001b[0;36mModel.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[1;32m    595\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, MLMODEL_FILE_NAME)\n\u001b[0;32m--> 596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(yaml\u001b[38;5;241m.\u001b[39msafe_load(f\u001b[38;5;241m.\u001b[39mread()))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sebastiensime/Documents/mlops-introduction/artifact_store/2/70aed8e7fafd493a8433054a2340ba05/artifacts/model/MLmodel'"
     ]
    }
   ],
   "source": [
    "# Load a specific model version\n",
    "# first register manually the model in MLflow UI\n",
    "model_name = \"agnews_pt_classifier\"\n",
    "model_version = \"1\"  # or \"production\", \"staging\"\n",
    "\n",
    "\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "model = mlflow.pytorch.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference\n",
    "\n",
    "Here, we define the `predict` function to perform inference using the loaded model. This function takes a list of texts, tokenizes them using a pre-trained tokenizer, and then feeds them into the model. The output is the model's prediction, which can be used for various applications such as text classification, sentiment analysis, etc. This step is crucial in demonstrating how a trained model can be utilized for practical applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(texts, model, tokenizer):\n",
    "    # Tokenize the texts\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Pass the inputs to the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Convert predictions to text labels\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    predictions = [model.config.id2label[prediction] for prediction in predictions]\n",
    "\n",
    "    # Print predictions\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text to predict\n",
    "texts = [\n",
    "    \"The local high school soccer team triumphed in the state championship, securing victory with a last-second winning goal.\",\n",
    "    \"DataCore is set to acquire startup InnovateAI for $2 billion, aiming to enhance its position in the artificial intelligence market.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer needs to be loaded sepparetly for this\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(predict(texts, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Method 2: Versatile Loading with Custom Handling\n",
    "\n",
    "This alternate method is more versatile and can handle different types of models. It's particularly useful when you're working with a variety of models or when the environment requires a more customized approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts:  20%|██        | 1/5 [00:00<00:00, 15887.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 46.04it/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/sebastiensime/Documents/mlops-introduction/artifact_store/2/70aed8e7fafd493a8433054a2340ba05/artifacts/model'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load custom model\n",
    "model_name = \"agnews-transformer\"\n",
    "model_version = \"1\"  # or \"production\", \"staging\"\n",
    "model_version_details = client.get_model_version(name=model_name, version=model_version)\n",
    "\n",
    "run_id = model_version_details.run_id\n",
    "artifact_path = model_version_details.source\n",
    "\n",
    "# Construct the model URI\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "\n",
    "model_path = \"models/agnews_transformer\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "client.download_artifacts(run_id, artifact_path, dst_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "custom_model = AutoModelForSequenceClassification.from_pretrained(\"models/agnews_transformer/model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/agnews_transformer/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sports', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Do the inference\n",
    "print(predict(texts, custom_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Model Versioning with MLflow\n",
    "\n",
    "One of the powerful features of MLflow is its ability to manage multiple versions of models. In this section, we log new iterations of our model to showcase this versioning capability. By setting a new experiment and logging models under different run names, we effectively create multiple versions of the same model. This is a crucial aspect of MLOps, as it allows for tracking the evolution of models over time, comparing different iterations, and systematically managing the model lifecycle. We demonstrate this by logging two additional iterations of our model, tagged as \"iteration2\" and \"iteration3\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log some new models for versioning demonstration\n",
    "mlflow.set_experiment(\"sequence_classification\")\n",
    "\n",
    "# Log a new model as iteration 2\n",
    "with mlflow.start_run(run_name=\"iteration2\"):\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "# Log another new model as iteration 3\n",
    "with mlflow.start_run(run_name=\"iteration3\"):\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference\n",
    "\n",
    "Here, we define the `predict` function to perform inference using the loaded model. This function takes a list of texts, tokenizes them using a pre-trained tokenizer, and then feeds them into the model. The output is the model's prediction, which can be used for various applications such as text classification, sentiment analysis, etc. This step is crucial in demonstrating how a trained model can be utilized for practical applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1, Stage: Production\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h3/0dj09bhd7gq2hmkhl8s80jsc0000gn/T/ipykernel_87980/2809489374.py:7: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.13.2/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(name=model_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1718305889784, current_stage='Production', description='', last_updated_timestamp=1718310503515, name='agnews-transformer', run_id='70aed8e7fafd493a8433054a2340ba05', run_link='', source='/Users/sebastiensime/Documents/mlops-introduction/artifact_store/2/70aed8e7fafd493a8433054a2340ba05/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model version management\n",
    "model_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "for version in model_versions:\n",
    "    print(f\"Version: {version.version}, Stage: {version.current_stage}\")\n",
    "\n",
    "# Change model stage\n",
    "client.transition_model_version_stage(name=model_name, \n",
    "                                      version=model_version, \n",
    "                                      stage=\"Production\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up: Deleting Models and Versions\n",
    "\n",
    "In some scenarios, you might need to delete specific model versions or even entire registered models from MLflow. This section covers how to perform these deletions. Note that this should be done cautiously, as it cannot be undone. This is particularly useful for maintaining a clean and efficient model registry by removing outdated or unused models and versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a specific model version\n",
    "client.delete_model_version(name=model_name, version=model_version)\n",
    "\n",
    "# Delete the entire registered model\n",
    "client.delete_registered_model(name=model_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
