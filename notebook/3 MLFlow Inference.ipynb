{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Integration for Model Serving and Registry Management\n",
    "\n",
    "In this notebook, we delve into advanced aspects of MLflow, focusing on model serving, inference, and the management of model versions in the MLflow model registry. Our goal is to demonstrate how MLflow supports the operational phase of the machine learning lifecycle, which includes serving models for inference and efficiently managing multiple versions of models.\n",
    "\n",
    "We will explore the practical application of these concepts using a text classification model. This will include loading models for inference, performing predictions, managing different versions of models, and understanding how to transition models through various stages in the model lifecycle. These skills are essential for operational efficiency and effective model management in real-world machine learning applications, aligning with the core themes of our course on MLops and experiment tracking.\n",
    "\n",
    "\n",
    "### Objective:\n",
    "* Loading and Serving Models\n",
    "* Inference with the Model\n",
    "* Managing Model Versions\n",
    "* Deleting Models and Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Ensure all necessary libraries are installed and imported for our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Import necessary libraries focusing on MLflow for model retrieval, PyTorch for model operations, and Transformers for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\Documents\\projet\\mlops-introduction\\mlflow_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# loading the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Mlflow Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI (local)\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with daghub\n",
    "REPO_OWNER = os.environ['MLFLOW_TRACKING_USERNAME']\n",
    "#os.environ['MLFLOW_TRACKING_PASSWORD'] = os.getenv('MLFLOW_TRACKING_PASSWORD')\n",
    "REPO_NAME = \"mlops-introduction\"\n",
    "mlflow.set_tracking_uri(f'https://dagshub.com/{REPO_OWNER}/{REPO_NAME}.mlflow')\n",
    "\n",
    "# set mlflow tracking url\n",
    "client = mlflow.tracking.MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Model from MLflow\n",
    "\n",
    "In this step, we'll explore two methods to retrieve our trained model from MLflow. Understanding the nuances of each method is key to making an informed choice in a real-life scenario based on the requirements and constraints of your deployment environment.\n",
    "\n",
    "#### Method 1: Using the Built-in PyTorch Loader\n",
    "\n",
    "This method is straightforward and uses MLflow's built-in functionality to load PyTorch models. It's user-friendly and works well when you're working within a PyTorch-centric workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 15/15 [00:30<00:00,  2.02s/it] \n",
      "2024/07/03 22:56:30 WARNING mlflow.pytorch: Stored model version '2.3.1+cu121' does not match installed PyTorch version '2.3.1+cpu'\n",
      "Downloading artifacts: 100%|██████████| 15/15 [00:30<00:00,  2.03s/it] \n",
      "2024/07/03 22:57:02 INFO mlflow.pyfunc: To install the dependencies that were used to train the model, run the following command: '%pip install -r C:\\Users\\sebas\\AppData\\Local\\Temp\\tmpvcq5cf9t\\model\\requirements.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Load a specific model version\n",
    "# first register manually the model in MLflow UI\n",
    "model_name = \"agnews_pt_classifier-v2\"\n",
    "model_version = \"1\"  # or \"production\", \"staging\"\n",
    "\n",
    "# load from local\n",
    "#model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "\n",
    "# load from daghub\n",
    "model_uri = \"mlflow-artifacts:/f1d16bb285c24918a9fae3a2612cdf38/fe91c1c1b9c848b5baacbd72b12efd56/artifacts/model\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_uri, map_location=torch.device('cpu'))\n",
    "\n",
    "env_file_path = mlflow.pyfunc.get_model_dependencies(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/04 13:20:26 INFO mlflow.pyfunc: To install the dependencies that were used to train the model, run the following command: '%pip install -r C:\\Users\\sebas\\Documents\\projet\\mlops-introduction\\artifact_store\\4\\238ec47ab76d403fbb95f1e0ce56fd28\\artifacts\\model\\requirements.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Load a specific model version\n",
    "# first register manually the model in MLflow UI\n",
    "model_name = \"agnews-transformer-v2\"\n",
    "model_version = \"1\"  # or \"production\", \"staging\"\n",
    "\n",
    "# load from local\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_uri, map_location=torch.device('cpu'))\n",
    "\n",
    "env_file_path = mlflow.pyfunc.get_model_dependencies(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference\n",
    "\n",
    "Here, we define the `predict` function to perform inference using the loaded model. This function takes a list of texts, tokenizes them using a pre-trained tokenizer, and then feeds them into the model. The output is the model's prediction, which can be used for various applications such as text classification, sentiment analysis, etc. This step is crucial in demonstrating how a trained model can be utilized for practical applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(texts, model, tokenizer):\n",
    "    # Tokenize the texts\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Pass the inputs to the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Convert predictions to text labels\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    predictions = [model.config.id2label[prediction] for prediction in predictions]\n",
    "\n",
    "    # Print predictions\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text to predict\n",
    "texts = [\n",
    "    \"The local high school soccer team triumphed in the state championship, securing victory with a last-second winning goal.\",\n",
    "    \"DataCore is set to acquire startup InnovateAI for $2 billion, aiming to enhance its position in the artificial intelligence market.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['World', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer needs to be loaded sepparetly for this\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(predict(texts, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2 (local only): Versatile Loading with Custom Handling\n",
    "\n",
    "This alternate method is more versatile and can handle different types of models. It's particularly useful when you're working with a variety of models or when the environment requires a more customized approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"agnews-transformer-v2\"\n",
    "model_version = \"1\"  # or \"production\", \"staging\"\n",
    "model_version_details = client.get_model_version(name=model_name, version=model_version)\n",
    "\n",
    "run_id = model_version_details.run_id\n",
    "artifact_path = model_version_details.source\n",
    "\n",
    "# Construct the model URI\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "\n",
    "model_path = \"models/agnews_transformer\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "client.download_artifacts(run_id, artifact_path, dst_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "custom_model = AutoModelForSequenceClassification.from_pretrained(\"models/agnews_transformer/model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/agnews_transformer/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sports', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Do the inference\n",
    "print(predict(texts, custom_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Model Versioning with MLflow\n",
    "\n",
    "One of the powerful features of MLflow is its ability to manage multiple versions of models. In this section, we log new iterations of our model to showcase this versioning capability. By setting a new experiment and logging models under different run names, we effectively create multiple versions of the same model. This is a crucial aspect of MLOps, as it allows for tracking the evolution of models over time, comparing different iterations, and systematically managing the model lifecycle. We demonstrate this by logging two additional iterations of our model, tagged as \"iteration2\" and \"iteration3\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/03 22:59:16 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/07/03 22:59:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\sebas\\AppData\\Local\\Temp\\tmpraj6_ukn\\model\\data, flavor: pytorch). Fall back to return ['torch==2.3.1', 'cloudpickle==3.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/07/03 23:00:38 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/07/03 23:00:47 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\sebas\\Documents\\projet\\mlops-introduction\\mlflow_env\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sebas\\Documents\\projet\\mlops-introduction\\mlflow_env\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Log some new models for versioning demonstration\n",
    "mlflow.set_experiment(\"sequence_classification\")\n",
    "\n",
    "# Log a new model as iteration 2\n",
    "with mlflow.start_run(run_name=\"iteration2\"):\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "# Log another new model as iteration 3\n",
    "with mlflow.start_run(run_name=\"iteration3\"):\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference\n",
    "\n",
    "Here, we define the `predict` function to perform inference using the loaded model. This function takes a list of texts, tokenizes them using a pre-trained tokenizer, and then feeds them into the model. The output is the model's prediction, which can be used for various applications such as text classification, sentiment analysis, etc. This step is crucial in demonstrating how a trained model can be utilized for practical applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1, Stage: Production\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h3/0dj09bhd7gq2hmkhl8s80jsc0000gn/T/ipykernel_87980/2809489374.py:7: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.13.2/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(name=model_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1718305889784, current_stage='Production', description='', last_updated_timestamp=1718310503515, name='agnews-transformer', run_id='70aed8e7fafd493a8433054a2340ba05', run_link='', source='/Users/sebastiensime/Documents/mlops-introduction/artifact_store/2/70aed8e7fafd493a8433054a2340ba05/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model version management\n",
    "model_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "for version in model_versions:\n",
    "    print(f\"Version: {version.version}, Stage: {version.current_stage}\")\n",
    "\n",
    "# Change model stage\n",
    "client.transition_model_version_stage(name=model_name, \n",
    "                                      version=model_version, \n",
    "                                      stage=\"Production\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up: Deleting Models and Versions\n",
    "\n",
    "In some scenarios, you might need to delete specific model versions or even entire registered models from MLflow. This section covers how to perform these deletions. Note that this should be done cautiously, as it cannot be undone. This is particularly useful for maintaining a clean and efficient model registry by removing outdated or unused models and versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a specific model version\n",
    "client.delete_model_version(name=model_name, version=model_version)\n",
    "\n",
    "# Delete the entire registered model\n",
    "client.delete_registered_model(name=model_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
