{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNq1zGborY75"
      },
      "source": [
        "## **1. Setting Up MLflow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GHQC-C0ZrY77"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "# Set the tracking URI for MLflow to the local server\n",
        "mlflow.set_tracking_uri(\"http://localhost:5000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DanIAq_NrY78"
      },
      "source": [
        "- **What is MLflow?**: MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It includes tools for tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\n",
        "\n",
        "\n",
        "- **Setting up MLflow**: The first step in using MLflow is to set up the tracking server, where all the experiment data will be stored. `mlflow.set_tracking_uri(\"http://localhost:5000\")` sets the tracking URI to a local server (running on localhost at port 5000). This means all the data from your experiments will be sent to this server for tracking and storage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cF1zwprrY78"
      },
      "source": [
        "## **2. Creating and Managing Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hTXnIOSErY78"
      },
      "outputs": [],
      "source": [
        "# Creating a new experiment\n",
        "experiment_id = mlflow.create_experiment(\"My New Experiment\")\n",
        "\n",
        "# Starting a new run using a context manager\n",
        "with mlflow.start_run(experiment_id=experiment_id):\n",
        "    # Your ML code goes here\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manually creating a custom named run\n",
        "run = mlflow.start_run(experiment_id=experiment_id, run_name=\"First run\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5a7tGberY78"
      },
      "source": [
        "- **Creating Experiments**: `mlflow.create_experiment(\"My New Experiment\")` creates a new experiment in MLflow. An experiment is a way to organize and keep track of your machine learning runs. Each experiment contains multiple runs.\n",
        "\n",
        "\n",
        "- **Starting Runs**: A \"run\" is a single execution of a machine learning code. MLflow allows you to start a run using two methods:\n",
        "    - **Context Manager**: The `with mlflow.start_run()` syntax automatically starts and ends a run. This is useful as it ensures the run is closed properly after the code block is executed.\n",
        "    - **Manual Management**: You can also start and end a run manually using `mlflow.start_run()` and `mlflow.end_run()`. This method gives you more control over when the run starts and ends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeDTauK7rY79"
      },
      "source": [
        "## **3. Logging Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NHXYQQYKrY79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Logging multiple parameters\n",
        "mlflow.log_param(\"learning_rate\", 0.01)\n",
        "mlflow.log_param(\"batch_size\", 32)\n",
        "num_epochs = 10\n",
        "mlflow.log_param(\"num_epocs\", num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "semd8NybrY79"
      },
      "source": [
        "\n",
        "- **Purpose of Logging Parameters**: Parameters are the configuration settings used for your machine learning model. Logging them helps you keep track of which settings were used in each run, which is crucial for experiment reproducibility and comparison.\n",
        "- **How it Works**: The `mlflow.log_param` function logs parameters like learning rate, batch size, and number of epochs. These parameters are then visible in the MLflow UI, allowing you to see how different configurations affect model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHfI5bw0rY79"
      },
      "source": [
        "## **4. Logging Metrics**\n",
        "\n",
        "Metrics are time based."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b2LWOWaZrY7-"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy\n",
        "import numpy as np\n",
        "\n",
        "# Logging metrics for each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    mlflow.log_metric(\"accuracy\", np.random.random(), step=epoch)\n",
        "    mlflow.log_metric(\"loss\", np.random.random(), step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logging a time-series metric\n",
        "for t in range(100):\n",
        "    metric_value = np.sin(t * np.pi / 50)\n",
        "    mlflow.log_metric(\"time_series_metric\", metric_value, step=t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACzpzbAprY7-"
      },
      "source": [
        "\n",
        "\n",
        "- **Metrics in Machine Learning**: Metrics are values that measure the performance of your model. Common metrics include accuracy and loss.\n",
        "- **Logging Metrics with MLflow**: `mlflow.log_metric` allows you to log these metrics during your training process. This is often done for each epoch (a single pass through the entire dataset), or step (a pass of a batch of data) to track how the model improves over time.\n",
        "- **Time-Series Metrics**: Besides typical metrics, you can also log custom metrics. In this example, a time-series metric based on a sine function is logged. This demonstrates how you can track any metric over time, which can be useful for more complex analyses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuEFHFburY7-"
      },
      "source": [
        "## **5. Logging Data and Artefacts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uNe3vMEnrY7-"
      },
      "outputs": [],
      "source": [
        "# Logging datasets\n",
        "with open(\"data/dataset.csv\", \"w\") as f:\n",
        "    f.write(\"x,y\\n\")\n",
        "    for x in range(100):\n",
        "        f.write(f\"{x},{x * 2}\\n\")\n",
        "\n",
        "mlflow.log_artifact(\"data/dataset.csv\", \"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMGdgrgXrY7-"
      },
      "source": [
        "### Exploring different types of artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BrddBvuyrY7-"
      },
      "outputs": [],
      "source": [
        "# !pip install plotly pandas\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Generate a confusion matrix\n",
        "confusion_matrix = np.random.randint(0, 100, size=(5, 5))  # 5x5 matrix\n",
        "\n",
        "labels = [\"Class A\", \"Class B\", \"Class C\", \"Class D\", \"Class E\"]\n",
        "df_cm = pd.DataFrame(confusion_matrix, index=labels, columns=labels)\n",
        "\n",
        "# Plot confusion matrix using Plotly Express\n",
        "fig = px.imshow(df_cm, text_auto=True, labels=dict(x=\"Predicted Label\", y=\"True Label\"), x=labels, y=labels, title=\"Confusion Matrix\")\n",
        "\n",
        "# Save the figure as an HTML file\n",
        "html_file = \"../reports/confusion_matrix.html\"\n",
        "fig.write_html(html_file)\n",
        "\n",
        "# Log the HTML file with MLflow\n",
        "mlflow.log_artifact(html_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbsbO0BurY7-"
      },
      "source": [
        "\n",
        "- **What are Artifacts?**: In MLflow, an artifact is any file or data that you want to log along with your run. This can include datasets, models, images, or even custom files.\n",
        "- **Logging Artifacts**: The `mlflow.log_artifact` function allows you to log these artifacts. In this example, a dataset and a confusion matrix (saved as an HTML file) are logged. Logging artifacts helps in ensuring that all relevant data and outputs are stored and easily accessible for each run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XzOzqQ4rY7-"
      },
      "source": [
        "## **6. Logging Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TTpIMVadrY7-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<mlflow.models.model.ModelInfo at 0x31eaf1df0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import mlflow\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a dummy dataset\n",
        "X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train a dummy model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Log the model manually (although auto-logging will also capture the model)\n",
        "mlflow.sklearn.log_model(model, \"logistic_regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WSprMhwrY7-"
      },
      "source": [
        "- **Importance of Logging Models**: Keeping track of the models used in different runs is critical. It helps in model comparison, versioning, and deployment.\n",
        "- **How to Log Models**: MLflow provides functions to log models from various machine learning frameworks. In this case, `mlflow.pytorch.log_model` is used to log a PyTorch model. This function saves the model in a format that can be easily reloaded for future predictions or analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjisqvu2rY7_"
      },
      "source": [
        "## **7. Ending the Run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "g1D66jB6rY7_"
      },
      "outputs": [],
      "source": [
        "# End run\n",
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-Xy-5PArY7_"
      },
      "source": [
        "- **What Does Ending a Run Mean?**: In MLflow, ending a run signifies the completion of a specific machine learning experiment or process. It marks the point where you have finished logging parameters, metrics, and artifacts for that particular execution of your model or script.\n",
        "\n",
        "- **Why is it Important?**: It helps in keeping your experiments organized. Each run is a separate record in MLflow. By ending a run, you ensure that all the data logged after this point will be part of a new run, keeping your experiment's data clean and segregated.\n",
        "\n",
        "- **How to End a Run**: You can end a run using `mlflow.end_run()`. This method is particularly important when you start a run without using a context manager (the `with` statement). With a context manager, the run is automatically ended when you exit the block of code inside the `with` statement. However, if you start a run manually using `mlflow.start_run()`, you should always ensure to call `mlflow.end_run()` once all logging is done.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
