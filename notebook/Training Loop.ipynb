{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgv98kwm5Vyr"
      },
      "source": [
        "\n",
        "## MLflow Integration for Model Training and Tracking\n",
        "\n",
        "In this notebook, we're integrating MLflow into a machine learning workflow to track and manage experiments effectively. We're focusing on a text classification task using the DistilBert model, emphasizing the importance of experiment tracking, model management, and operational efficiency - core themes of our course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0X5uyaQ5Vyt"
      },
      "source": [
        "### Objective:\n",
        "\n",
        "- Dynamically set up and log parameters to MLflow\n",
        "- Understand the purpose and application of each step in the context of MLflow and MLOps principles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuSiMFnc5Vyt"
      },
      "source": [
        "### Environment Setup\n",
        "\n",
        "Ensure all necessary libraries are installed and imported for our workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zebzdop25Vyu"
      },
      "source": [
        "\n",
        "### Imports\n",
        "\n",
        "Import the necessary libraries, focusing on MLflow for tracking, PyTorch for model training, and Transformers for our NLP model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p0eWzvXf5Vyu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sebastiensime/Documents/mlops-introduction/mlflow_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# !pip install datasets\n",
        "import os\n",
        "import mlflow\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, AdamW\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix-cE3OS5Vyu"
      },
      "source": [
        "### Configuration Parameters as an Object\n",
        "\n",
        "By defining parameters as a dictionary, we can easily iterate through them when logging to MLflow. This method streamlines the process and adheres to best practices in code maintainability and scalability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qZ3uT3ED5Vyv"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'model_name': 'distilbert-base-cased',\n",
        "    'learning_rate': 5e-5,\n",
        "    'batch_size': 16,\n",
        "    'num_epochs': 1,\n",
        "    'dataset_name': 'ag_news',\n",
        "    'task_name': 'sequence_classification',\n",
        "    'log_steps': 100,\n",
        "    'max_seq_length': 128,\n",
        "    'output_dir': 'models/distilbert-base-uncased-ag_news',\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUuBiFwN5Vyv"
      },
      "source": [
        "\n",
        "### MLflow Setup\n",
        "\n",
        "Setting up MLflow is crucial for tracking our experiments, parameters, and results, allowing us to manage and compare different runs effectively - a practice that aligns with the MLOps goal of systematic and efficient model management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PH8r6mhd5Vyv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/06/13 20:47:48 INFO mlflow.tracking.fluent: Experiment with name 'sequence_classification' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='/Users/sebastiensime/Documents/mlops-introduction/artifact_store/2', creation_time=1718304468092, experiment_id='2', last_update_time=1718304468092, lifecycle_stage='active', name='sequence_classification', tags={}>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "mlflow.set_experiment(f\"{params['task_name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miIwBdFE5Vyw"
      },
      "source": [
        "### Load and Preprocess Dataset\n",
        "\n",
        "We're using a well-known NLP dataset to ensure reproducibility and comparability. The preprocessing step is crucial for converting raw text into a format that our model can understand, highlighting the importance of data preparation in the ML pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tSp0aciO5Vyw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 18.6M/18.6M [00:00<00:00, 22.7MB/s]\n",
            "Downloading data: 100%|██████████| 1.23M/1.23M [00:00<00:00, 4.03MB/s]\n",
            "Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 1818372.73 examples/s]\n",
            "Generating test split: 100%|██████████| 7600/7600 [00:00<00:00, 1939798.60 examples/s]\n",
            "/Users/sebastiensime/Documents/mlops-introduction/mlflow_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 20000/20000 [00:04<00:00, 4759.08 examples/s]\n",
            "Map: 100%|██████████| 2000/2000 [00:00<00:00, 4633.11 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess dataset\n",
        "dataset = load_dataset(params['dataset_name'])#, params['task_name'])\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(params['model_name'])\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=params['max_seq_length'])\n",
        "\n",
        "\n",
        "train_dataset = dataset[\"train\"].shuffle().select(range(20_000)).map(tokenize, batched=True)\n",
        "test_dataset = dataset[\"test\"].shuffle().select(range(2_000)).map(tokenize, batched=True)\n",
        "\n",
        "# Set format for PyTorch and create data loaders\n",
        "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "# get the labels\n",
        "labels = dataset[\"train\"].features['label'].names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0IWkrp85Vyw"
      },
      "source": [
        "\n",
        "### Model Initialization\n",
        "\n",
        "Initializing the model is a foundational step, showcasing the practical application of a pre-trained NLP model for a specific task - reflecting the course's focus on real-world applicability of machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h_uVy4Xc5Vyw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained(params['model_name'], \n",
        "                                                            num_labels=len(labels))\n",
        "model.config.id2label = {i: label for i, label in enumerate(labels)}\n",
        "params['id2label'] = model.config.id2label\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2p9VqT_5Vyw"
      },
      "source": [
        "### Optimizer Setup\n",
        "\n",
        "Choosing the right optimizer and learning rate is vital for effective model training. It demonstrates the importance of hyperparameter tuning, a key concept in achieving optimal model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LKN9psfz5Vyw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sebastiensime/Documents/mlops-introduction/mlflow_env/lib/python3.12/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=params['learning_rate'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkqHGboA5Vyw"
      },
      "source": [
        "### Evaluation Function\n",
        "\n",
        "Evaluating the model on a separate test set helps us understand its performance on unseen data, highlighting the concept of generalization which is crucial for real-world applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EasITvPx5Vyw"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs, masks, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs = model(inputs, attention_mask=masks)\n",
        "            logits = outputs.logits\n",
        "            _, predicted_labels = torch.max(logits, dim=1)\n",
        "\n",
        "            predictions.extend(predicted_labels.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate Evaluation Metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
        "\n",
        "    return accuracy, precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa0PjEfq5Vyx"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "The training loop is where the actual model training happens. Logging metrics and parameters at each step is crucial for tracking the model's progress, understanding its behavior, and making informed decisions - core aspects of the MLOps lifecycle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B7EFQOe15Vyx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/1] - (Loss: 0.241) - Steps: 100%|██████████| 1250/1250 [16:24<00:00,  1.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Metrics: Accuracy: 0.9085, Precision: 0.9107, Recall: 0.9094, F1: 0.9081\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully registered model 'agnews-transformer'.\n",
            "2024/06/13 21:11:29 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: agnews-transformer, version 1\n",
            "Created version '1' of model 'agnews-transformer'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Start MLflow Run\n",
        "with mlflow.start_run(run_name=f\"{params['model_name']}-{params['dataset_name']}\") as run:\n",
        "\n",
        "    # Log all parameters at once\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    with tqdm(total=params['num_epochs'] * len(train_loader), desc=f\"Epoch [1/{params['num_epochs']}] - (Loss: N/A) - Steps\") as pbar:\n",
        "        for epoch in range(params['num_epochs']):\n",
        "            running_loss = 0.0\n",
        "            for i, batch in enumerate(train_loader, 0):\n",
        "                inputs, masks, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                if i and i % params['log_steps'] == 0:\n",
        "                    avg_loss = running_loss / params['log_steps']\n",
        "\n",
        "                    pbar.set_description(f\"Epoch [{epoch + 1}/{params['num_epochs']}] - (Loss: {avg_loss:.3f}) - Steps\")\n",
        "                    mlflow.log_metric(\"loss\", avg_loss, step=epoch * len(train_loader) + i)\n",
        "\n",
        "                    running_loss = 0.0\n",
        "                pbar.update(1)\n",
        "\n",
        "            # Evaluate Model\n",
        "            accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device)\n",
        "            print(f\"Epoch {epoch + 1} Metrics: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "            # Log metrics to MLflow\n",
        "            mlflow.log_metrics({'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}, step=epoch)\n",
        "\n",
        "\n",
        "    # Log model to MLflow through built-in PyTorch method\n",
        "    # mlflow.pytorch.log_model(model, \"model\")\n",
        "\n",
        "    # Log model to MLflow through custom method\n",
        "    os.makedirs(params['output_dir'], exist_ok=True)\n",
        "    model.save_pretrained(params['output_dir'])\n",
        "    tokenizer.save_pretrained(params['output_dir'])\n",
        "\n",
        "    mlflow.log_artifacts(params['output_dir'], artifact_path=\"model\")\n",
        "\n",
        "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
        "    mlflow.register_model(model_uri, \"agnews-transformer\")\n",
        "\n",
        "print('Finished Training')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
